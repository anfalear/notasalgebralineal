\chapter{Valores y vectores propios}

\begin{definition}[Valores y vectores propios]
	Sea $A$ una matriz cuadrada $n\times n$. Un vector no nulo $\mathbf{v}\in \mathbb{R}^n$ es llamado \textbf{vector propio} (también conocido como \textbf{autovector} o \textbf{eigenvector}) de la matriz $A$ asociado al \textbf{valor propio} $\lambda \in \mathbb{R}$ (también llamado \textbf{autovalor} o \textbf{eigenvalor}) si satisface la ecuación:
	$$\boxed{A\mathbf{v} = \lambda \mathbf{v}}$$
	
	Reescribiendo esta ecuación, obtenemos: $\mathbf{0} = A\mathbf{v} - \lambda \mathbf{v} = (A - \lambda I_n)\mathbf{v}.$
	
	Para que existan vectores propios no nulos $\mathbf{v}$, la matriz $(A - \lambda I_n)$ debe ser singular, es decir:
	$$\det(A - \lambda I_n) = 0$$
	
	Esta ecuación es conocida como la \textbf{ecuación característica de la matriz $A$}. Sus soluciones $\lambda$ son los valores propios de $A$.
	
	Para cada valor propio $\lambda_i$, los vectores propios correspondientes forman el espacio nulo de la matriz $(A - \lambda_i I_n)$, es decir: $E_{\lambda_i} = \ker(A - \lambda_i I_n) = \{\mathbf{v} \in \mathbb{R}^n : (A - \lambda_i I_n)\mathbf{v} = \mathbf{0}\}.$
\end{definition}

\begin{example}\label{ex:ejemplovvp}
	Sea $A = \begin{pmatrix} 1 & 2 \\ 3 & 2 \end{pmatrix}$. Calcule sus valores propios.
	\begin{myproof}
	\(	\det(\lambda I - A) = \det\begin{pmatrix} \lambda - 1 & -2 \\ -3 & \lambda - 2 \end{pmatrix} = 0 \Rightarrow (\lambda - 1)(\lambda - 2) - (-2)(-3) = \lambda^2 - 3\lambda - 4 = 0.\)
	
	Así, \(\lambda = \frac{3 \pm \sqrt{9 + 16}}{2} = \frac{3 \pm 5}{2} \quad \Rightarrow \quad \lambda_1 = 4, \quad \lambda_2 = -1.	\) 	Por lo tanto, los valores propios de $A$ son $-1$ y $4$.
\end{myproof} 	
	
\end{example}

Según el teorema fundamental del álgebra (Teorema \ref{tfam}), todo polinomio no constante de grado $n$ con coeficientes complejos tiene exactamente $n$ raíces complejas contando multiplicidades. En el contexto del álgebra lineal, los valores $\lambda$ que son raíces del polinomio característico de $A$ se denominan \textbf{valores propios} de $A$.

\begin{definition}[Multiplicidad algebraica y geométrica]\label{multiplicidadgeometrica}
Para cada valor propio $\lambda$ de una matriz $A$, se definen:
\begin{itemize}
    \item \textbf{Multiplicidad algebraica}: Número de veces que $\lambda$ aparece como raíz del polinomio característico $\det(\lambda I - A) = 0$.
    \item \textbf{Multiplicidad geométrica}: Dimensión del espacio propio asociado a $\lambda$, es decir: \(\dim \ker(\lambda I - A).\)
\end{itemize}
\end{definition}

\begin{rem}
En el ejemplo \ref{ex:ejemplovvp} con $A = \begin{pmatrix} 1 & 2 \\ 3 & 2 \end{pmatrix}$, ambos valores propios ($\lambda = -1$ y $\lambda = 4$) tienen multiplicidad algebraica 1 y geométrica 1. 

Considere ahora la matriz $B = \begin{pmatrix} a & 0 \\ 0 & a \end{pmatrix}$. Su polinomio característico es \[
\det(\lambda I - B) = \det\begin{pmatrix} \lambda - a & 0 \\ 0 & \lambda - a \end{pmatrix} = (\lambda - a)^2 \], la única raíz es $\lambda = a$ con multiplicidad algebraica 2 y su multiplicidad geométrica también es 2, pues \(
\ker(aI - B) = \ker\begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} = \mathbb{R}^2
\) que tiene dimensión 2.
\end{rem}




\begin{theorem}[Caracterización de valores propios]\label{teo:caracterizacion}
Sea $A \in M_n(\mathbb{R})$. Las siguientes afirmaciones son equivalentes:
\begin{enumerate}[$(1)$]
    \item $\lambda$ es un valor propio de $A$.
    \item $\lambda$ es solución de la ecuación característica $\det(\lambda I - A) = 0$.
    \item Existe un vector no nulo $\mathbf{x} \in \mathbb{R}^n$ tal que $A\mathbf{x} = \lambda \mathbf{x}$.
\end{enumerate}
\end{theorem}

\begin{proof}
Demostramos las equivalencias:

\textbf{$(1)$) $\Rightarrow$ ($(3)$)}: Por definición de valor propio, existe $\mathbf{x} \neq \mathbf{0}$ tal que $A\mathbf{x} = \lambda\mathbf{x}$.

\textbf{$(3)$ $\Rightarrow$ ($(2)$)}: Si $A\mathbf{x} = \lambda\mathbf{x}$, entonces $(\lambda I - A)\mathbf{x} = \mathbf{0}$. Como $\mathbf{x} \neq \mathbf{0}$, la matriz $\lambda I - A$ es singular, luego $\det(\lambda I - A) = 0$.

\textbf{$(2)$ $\Rightarrow$ $(1)$}: Si $\det(\lambda I - A) = 0$, entonces $\lambda I - A$ es singular y existe $\mathbf{x} \neq \mathbf{0}$ tal que $(\lambda I - A)\mathbf{x} = \mathbf{0}$, es decir, $A\mathbf{x} = \lambda\mathbf{x}$.
\end{proof}

\begin{coro}[Valores propios de matrices triangulares]\label{cor:triangular}
Si $A$ es una matriz triangular, sus valores propios son los elementos de su diagonal principal.
\end{coro}

\begin{proof}
Para una matriz triangular $A = (a_{ij})$, el polinomio característico es:
\[
\det(\lambda I - A) = \prod_{i=1}^n (\lambda - a_{ii})
\]
Las raíces son $\lambda_i = a_{ii}$ para $i=1,\dots,n$.
\end{proof}


\begin{proof}
Por el Teorema \ref{teo:caracterizacion}: 
\[
\det(A) = 0 \iff \det(0 \cdot I - A) = 0 \iff \lambda = 0 \text{ es valor propio}
\]
Como $A$ es invertible si y solo si $\det(A) \neq 0$, el resultado sigue.
\end{proof}

\begin{theorem}[Potencia de valores propios]\label{teo:potencia}
Si $\lambda$ es valor propio de $A \in M_n(\mathbb{R})$ con vector propio asociado $\mathbf{x}$, entonces para todo $k \in \mathbb{Z}^+$, $\lambda^k$ es valor propio de $A^k$ con el mismo vector propio $\mathbf{x}$.
\end{theorem}

\begin{proof}
Por inducción sobre $k$:

\textbf{Caso base} ($k=1$): Por hipótesis, $A\mathbf{x} = \lambda\mathbf{x}$.

\textbf{Paso inductivo}: Supongamos $A^k\mathbf{x} = \lambda^k\mathbf{x}$. Entonces:
\[
A^{k+1}\mathbf{x} = A(A^k\mathbf{x}) = A(\lambda^k\mathbf{x}) = \lambda^k(A\mathbf{x}) = \lambda^k(\lambda\mathbf{x}) = \lambda^{k+1}\mathbf{x}
\]
Por inducción, el resultado vale para todo $k \in \mathbb{Z}^+$.
\end{proof}

\begin{example}
Sea $A$ una matriz con valor propio $\lambda_1 = 3$ y vector propio asociado $\mathbf{x} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$. Por el Teorema \ref{teo:potencia}, $A^5$ tiene valor propio $\lambda_1^5 = 3^5 = 243$ con el mismo vector propio $\mathbf{x}$.
\end{example}


\begin{prob}\label{prob:vvp} Dadas las siguientes matrices calcule sus valores y vectores propios, determine las multiplicidades algebraicas y geométricas correspondientes a cada valor propio.  
  \begin{enumerate}[$a)$]
  \item $A=\left( \begin{array}{ccc} 
	1&2&2 \\
	0&2&1\\
	-1&2&2\\
	\end{array} \right).$ 
\begin{myproof}
Cálculo del polinomio característico: \(
\det(\lambda I - A) = \det\begin{pmatrix}
\lambda-1 & -2 & -2 \\
0 & \lambda-2 & -1 \\
1 & -2 & \lambda-2
\end{pmatrix}
\)

Aplicamos operaciones elementales:
\begin{align*}
& \xrightarrow{f_3 + f_1 \to f_3} \det\begin{pmatrix}
\lambda-1 & -2 & -2 \\
0 & \lambda-2 & -1 \\
\lambda & -4 & \lambda-4
\end{pmatrix} \\
& \xrightarrow{f_3 - \lambda f_2 \to f_3} \det\begin{pmatrix}
\lambda-1 & -2 & -2 \\
0 & \lambda-2 & -1 \\
0 & -4 + \lambda(2-\lambda) & \lambda-4 + \lambda
\end{pmatrix}
\end{align*}

Desarrollando por cofactores:
\[
= (\lambda-1)\left[(\lambda-2)^2 - 2\right] + 2\left[0 + 1\right] - 2\left[0 - (\lambda-2)\right] = \lambda^3 - 5\lambda^2 + 8\lambda - 4
\]

Raíces: $\lambda = 1$ (multiplicidad algebraica 1), $\lambda = 2$ (multiplicidad algebraica 2).

\textbf{Eigenvalor $\lambda=1$}:
\[
( I - A) = \begin{pmatrix}
0 & -2 & -2 \\
0 & -1 & -1 \\
1 & -2 & -1
\end{pmatrix} \xrightarrow{\text{forma escalonada}} \begin{pmatrix}
1 & -2 & -1 \\
0 & 1 & 1 \\
0 & 0 & 0
\end{pmatrix}
\]
Solución: $v_1 = \begin{pmatrix} -1 \\ -1 \\ 1 \end{pmatrix}$ (multiplicidad geométrica 1).

\textbf{Eigenvalor $\lambda=2$}:
\[
(2I - A) = \begin{pmatrix}
1 & -2 & -2 \\
0 & 0 & -1 \\
1 & -2 & 0
\end{pmatrix} \xrightarrow{f_3 - f_1 \to f_3} \begin{pmatrix}
1 & -2 & -2 \\
0 & 0 & -1 \\
0 & 0 & 2
\end{pmatrix}
\]
Solución: $v_2 = \begin{pmatrix} 2 \\ 1 \\ 0 \end{pmatrix}, v_3 = \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix}$ (multiplicidad geométrica 2).
\end{myproof}
	
	\item $A=\left( \begin{array}{ccc} 
	1&2&4 \\
	0&2&3\\
	0&0&5\\
	\end{array} \right).$ 
\begin{myproof}
Matriz triangular superior, eigenvalores en diagonal: 
$\lambda_1 = 1$ (mult. alg. 1), $\lambda_2 = 2$ (mult. alg. 1), $\lambda_3 = 5$ (mult. alg. 1).

\textbf{Eigenvalor $\lambda=1$}:
\[
(I - A) = \begin{pmatrix}
0 & -2 & -4 \\
0 & -1 & -3 \\
0 & 0 & -4
\end{pmatrix} \xrightarrow{\text{forma escalonada}} \begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{pmatrix}
\]
Solución: $v_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$ (mult. geom. 1).

\textbf{Eigenvalor $\lambda=2$}:
\[
(2I - A) = \begin{pmatrix}
1 & -2 & -4 \\
0 & 0 & -3 \\
0 & 0 & -3
\end{pmatrix} \xrightarrow{f_3 - f_2 \to f_3} \begin{pmatrix}
1 & -2 & -4 \\
0 & 0 & -3 \\
0 & 0 & 0
\end{pmatrix}
\]
Solución: $v_2 = \begin{pmatrix} 2 \\ 1 \\ 0 \end{pmatrix}$ (mult. geom. 1).

\textbf{Eigenvalor $\lambda=5$}:
\[
(5I - A) = \begin{pmatrix}
4 & -2 & -4 \\
0 & 3 & -3 \\
0 & 0 & 0
\end{pmatrix} \xrightarrow{f_2/3 \to f_2} \begin{pmatrix}
4 & -2 & -4 \\
0 & 1 & -1 \\
0 & 0 & 0
\end{pmatrix}
\]
Solución: $v_3 = \begin{pmatrix} 3/2 \\ 1 \\ 1 \end{pmatrix}$ (mult. geom. 1).
\end{myproof}
	
	\item $A=\left(\begin{matrix}
-10 & -71 & -19 \\
3 & 34 & 9 \\
-1 & -61 & -16
\end{matrix}\right)$ 
\begin{myproof}
Polinomio característico: $\det(\lambda I - A) = \lambda^3 - 8\lambda^2 + 19\lambda - 12 = 0$. \\
Raíces: $\lambda_1=1$, $\lambda_2=3$, $\lambda_3=4$ (mult. alg. 1 cada una).

\textbf{Eigenvalor $\lambda=1$}:
\[
(I - A) = \begin{pmatrix}
11 & 71 & 19 \\
-3 & -33 & -9 \\
1 & 61 & 17
\end{pmatrix} \xrightarrow{\text{reducción}} \begin{pmatrix}
1 & 0 & \frac{-2}{25} \\
0 & 1 & \frac{7}{25} \\
0 & 0& 0
\end{pmatrix} \to \cdots
\]
Solución: $v_1 = \begin{pmatrix} 2/25 \\ -7/25 \\ 1 \end{pmatrix}$ (mult. geom. 1).

\textbf{Eigenvalor $\lambda=3$}:
\[
(3I - A) = \begin{pmatrix}
13 & 71 & 19 \\
-3 & -31 & -9 \\
1 & 61 & 19
\end{pmatrix} \xrightarrow{\text{reducción}} \begin{pmatrix}
1 & 0 & 5/19 \\
0 & 1 & -6/19 \\
0 & 0 & 0
\end{pmatrix}
\]
Solución: $v_2 = \begin{pmatrix} 5/19 \\ -6/19 \\ 1 \end{pmatrix}$ (mult. geom. 1).

\textbf{Eigenvalor $\lambda=4$}:
\[
(4I - A) = \begin{pmatrix}
14 & 71 & 19 \\
-3 & -30 & -9 \\
1 & 61 & 20
\end{pmatrix} \xrightarrow{\text{reducción}} \begin{pmatrix}
1 & 0 & 1/3 \\
0 & 1 & -1/3 \\
0 & 0 & 0
\end{pmatrix}
\]
Solución: $v_3 = \begin{pmatrix} 1/3 \\ -1/3 \\ 1 \end{pmatrix}$ (mult. geom. 1).
\end{myproof}

\item $A=\left(\begin{matrix}
4 & 1 & 0 & 1 \\
2 & 3 & 0 & 1 \\
-2 & 1 & 2 & -3 \\
2 & -1 & 0 & 5
\end{matrix}\right)$
\begin{myproof}
Polinomio característico: $(\lambda-2)^2(\lambda-4)(\lambda-6)=0$. \\
Eigenvalores: $\lambda_1=2$ (mult. alg. 2), $\lambda_2=4$ (mult. alg. 1), $\lambda_3=6$ (mult. alg. 1).

\textbf{Eigenvalor $\lambda=2$}:
\[
(2I - A) = \begin{pmatrix}
-2 & -1 & 0 & -1 \\
-2 & -1 & 0 & -1 \\
2 & -1 & 0 & 3 \\
-2 & 1 & 0 & -3
\end{pmatrix} \xrightarrow{\text{redución}} \begin{pmatrix}
1 & 0 & 0 & 1 \\
0 & 1 & 0 & -1 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{pmatrix}
\]
Solución: $v_1 = \begin{pmatrix} -1 \\ 1 \\ 0 \\ 1 \end{pmatrix}, v_2 = \begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix}$ (mult. geom. 2).

\textbf{Eigenvalor $\lambda=4$}:
\[
(4I - A) = \begin{pmatrix}
0 & -1 & 0 & -1 \\
-2 & 1 & 0 & -1 \\
2 & -1 & 2 & 3 \\
-2 & 1 & 0 & -1
\end{pmatrix} \xrightarrow{\text{reducción}} \begin{pmatrix}
1 & 0 & 0 & 1 \\
0 & 1 & 0 & 1 \\
0 & 0 & 1 & 1 \\
0 & 0 & 0 & 0
\end{pmatrix}
\]
Solución: $v_3 = \begin{pmatrix} -1 \\ -1 \\ -1 \\ 1 \end{pmatrix}$ (mult. geom. 1).

\textbf{Eigenvalor $\lambda=6$}:
\[
(6I - A) = \begin{pmatrix}
2 & -1 & 0 & -1 \\
-2 & 3 & 0 & -1 \\
2 & -1 & 4 & 3 \\
-2 & 1 & 0 & 1
\end{pmatrix} \xrightarrow{\text{reducción}} \begin{pmatrix}
1 & 0 & 0 & -1 \\
0 & 1 & 0 & -1 \\
0 & 0 & 1 & 1 \\
0 & 0 & 0 & 0
\end{pmatrix}
\]
Solución: $v_4 = \begin{pmatrix} 1 \\ 1 \\ -1 \\ 1 \end{pmatrix}$ (mult. geom. 1).
\end{myproof}
  \end{enumerate}  
\end{prob}


\begin{rem}[¿Qué ocurre si los valores propios $\lambda$ son complejos?]
Consideremos el ejemplo $K = \begin{pmatrix} 2 & 3 \\ -3 & 2 \end{pmatrix}$. El polinomio característico es
\[
\det(\lambda I - K) = (\lambda - 2)^2 + 9 = 0,
\]
cuyas soluciones son $\lambda_1 = 2 + 3i$ y $\lambda_2 = 2 - 3i$.

De acuerdo con los conceptos de números complejos (véase la referencia correspondiente), el módulo de cada valor propio es
\[
|\lambda| = \sqrt{2^2 + 3^2} = \sqrt{13},
\]
y su argumento es
\[
\arg(\lambda) = \arctan\left(\frac{3}{2}\right).
\]

Estos valores propios complejos permiten interpretar la acción de la matriz como una combinación de rotación y dilatación en el plano, lo cual es fundamental en el estudio de transformaciones lineales y sistemas dinámicos.
\end{rem}




\begin{definition}[Matriz diagonalizable]
Una matriz $A \in M_n(\mathbb{R})$ es \textbf{diagonalizable} si existe una matriz invertible $P$ y una matriz diagonal $D$ tales que: \(A = PDP^{-1}.\)
\end{definition}


\begin{theorem}[Condición de diagonalización]\label{teo:diagonalizacion}
Sea $A \in M_n(\mathbb{R})$. Las siguientes afirmaciones son equivalentes:
\begin{enumerate}[$(1)$]
    \item $A$ es diagonalizable.
    \item Para cada valor propio $\lambda$ de $A$, la multiplicidad geométrica coincide con la multiplicidad algebraica.
    \item $A$ tiene $n$ vectores propios linealmente independientes.
    \item La suma de las multiplicidades geométricas de todos los valores propios es $n$.
\end{enumerate}



\begin{proof} \textbf{(1) $\Rightarrow$ (2):} Si $A$ es diagonalizable, existe $P$ invertible y $D$ diagonal tal que $A = PDP^{-1}$. Las columnas de $P$ son vectores propios que forman una base de $\mathbb{R}^n$. Para cada valor propio $\lambda_i$ con multiplicidad algebraica $m_i$, la matriz $D$ contiene $m_i$ entradas $\lambda_i$. Como los vectores propios son l.i., la dimensión del espacio propio es $m_i$, luego la multiplicidad geométrica es $m_i$.

\textbf{(2) $\Rightarrow$ (3):} Si para cada valor propio $\lambda_i$ la multiplicidad geométrica ($d_i$) iguala a la algebraica ($m_i$), entonces $\sum d_i = \sum m_i = n$. Los espacios propios generan $\mathbb{R}^n$ y podemos construir una base de $\mathbb{R}^n$ con $n$ vectores propios l.i.

\textbf{(3) $\Rightarrow$ (1):} Si $\{v_1,\dots,v_n\}$ son vectores propios l.i. asociados a $\lambda_1,\dots,\lambda_n$ (no necesariamente distintos), sea $P$ la matriz con columnas $v_i$ y $D = \text{diag}(\lambda_1,\dots,\lambda_n)$. Entonces $AP = PD$, y como $P$ es invertible, $A = PDP^{-1}$.

\textbf{(2) $\Leftrightarrow$ (4):} Como $\sum m_i = n$ siempre, entonces $\sum d_i = n$ si y solo si $d_i = m_i$ para todo $i$.
\end{proof}
\end{theorem}
\begin{theorem} Si $A$ tiene $n$ valores propios distintos, entonces $A$ es diagonalizable.
\begin{proof}
Si $A$ tiene $n$ valores propios distintos, cada multiplicidad algebraica es 1. Como $1 \leq d_i \leq m_i = 1$, entonces $d_i = 1$ para todo $i$, luego $A$ es diagonalizable. Los vectores propios asociados a valores propios distintos son linealmente independientes.
\end{proof}
\end{theorem}


\begin{example}
Diagonalice la matriz $A=\left(\begin{matrix}
-10 & -71 & -19 \\
3 & 34 & 9 \\
-1 & -61 & -16
\end{matrix}\right).$
\begin{myproof}
Ya se calcularon los valores y vectores propios en el problema \ref{prob:vvp}, obteniendo:

Polinomio característico: $\det(\lambda I - A) = \lambda^3 - 8\lambda^2 + 19\lambda - 12 = 0$ y sus raíces: $\lambda_1=1$, $\lambda_2=3$, $\lambda_3=4$ (todas de multiplicidad algebraica 1).

Los vectores propios asociados son:
\[
v_1 = \begin{pmatrix} \frac{2}{25} \\ -\frac{7}{25} \\ 1 \end{pmatrix}, \quad
v_2 = \begin{pmatrix} \frac{5}{19} \\ -\frac{6}{19} \\ 1 \end{pmatrix}, \quad
v_3 = \begin{pmatrix} \frac{1}{3} \\ -\frac{1}{3} \\ 1 \end{pmatrix}
\]

Construimos la matriz $P$ con estos vectores propios como columnas:
\[
P = \begin{pmatrix}
\frac{2}{25} & \frac{5}{19} & \frac{1}{3} \\
-\frac{7}{25} & -\frac{6}{19} & -\frac{1}{3} \\
1 & 1 & 1
\end{pmatrix}
\]

La matriz diagonal $D$ contiene los valores propios en la diagonal:
\[
D = \begin{pmatrix}
1 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & 4
\end{pmatrix}
\]

Por lo tanto, la diagonalización de $A$ es:
\[
A = P D P^{-1}
\]
donde $P$ es invertible y $D$ es diagonal.

\textbf{Verificación:} Calculamos la inversa de \(P\):
\[
P^{-1} = \begin{pmatrix}
-25 & -100 & -25 \\
76 & 361 & 95 \\
-51 & -261 & -69
\end{pmatrix}
\]

y así se puede verificar que \(A = P D P^{-1}\):
\[
P D P^{-1} = \begin{pmatrix}
-10 & -71 & -19 \\
3 & 34 & 9 \\
-1 & -61 & -16
\end{pmatrix} = A
\]
\end{myproof}
\end{example}






\begin{rem}[Cálculo de potencias de una matriz]\label{potdim}
Sea $A$ una matriz diagonalizable, es decir, existe una matriz invertible $P$ y una matriz diagonal $D$ tal que $A = P D P^{-1}$. Entonces, para cualquier entero positivo $k$, se cumple:
\[
A^k = (P D P^{-1})^k = P D^k P^{-1}
\]
Esto se debe a que, al multiplicar $A$ por sí misma $k$ veces, las matrices $P^{-1}P$ se simplifican sucesivamente, resultando en la expresión compacta:
\[
A^k = \underbrace{(P D P^{-1})(P D P^{-1}) \cdots (P D P^{-1})}_{k \text{ veces}} = P D^k P^{-1}
\]
\end{rem}

\begin{example}
Suponga que $(1,1)$ es un vector propio de la matriz $A$ correspondiente al valor propio $3$ y que $(2,1)$ es un vector propio correspondiente al valor propio $-2$. Calcula $A^6 \begin{pmatrix} 4 \\ 3 \end{pmatrix}$.
\begin{myproof}
Aplicando el resultado sobre potencias de matrices diagonalizables (ver Observación~\ref{potdim}), como
\[
P = \begin{pmatrix} 1 & 2 \\ 1 & 1 \end{pmatrix}, \quad D = \begin{pmatrix} 3 & 0 \\ 0 & -2 \end{pmatrix},
\]
tenemos:
\[
A^6 = (P D P^{-1})^6 = P D^6 P^{-1}
\]
Calculamos $D^6$:
\[
D^6 = \begin{pmatrix} 3^6 & 0 \\ 0 & (-2)^6 \end{pmatrix} = \begin{pmatrix} 729 & 0 \\ 0 & 64 \end{pmatrix}
\]
La inversa de $P$ es:
\[
P^{-1} = \begin{pmatrix} -1 & 2 \\ 1 & -1 \end{pmatrix}
\]
Por lo tanto,
\begin{align*}
A^6 &= P D^6 P^{-1} = 
\begin{pmatrix} 1 & 2 \\ 1 & 1 \end{pmatrix}
\begin{pmatrix} 729 & 0 \\ 0 & 64 \end{pmatrix}
\begin{pmatrix} -1 & 2 \\ 1 & -1 \end{pmatrix} \\
&= \begin{pmatrix} -601 & 1330 \\ -665 & 1394 \end{pmatrix}
\end{align*}
Finalmente,
\[
A^6 \begin{pmatrix} 4 \\ 3 \end{pmatrix}
= \begin{pmatrix} -601 & 1330 \\ -665 & 1394 \end{pmatrix} \begin{pmatrix} 4 \\ 3 \end{pmatrix}
= \begin{pmatrix} 1586 \\ 1522 \end{pmatrix}
\]
\end{myproof}
\end{example}


\begin{prob}[Solución de ecuaciones diferenciales de primer orden con coeficientes constantes]
Una ecuación diferencial (E.D.) es una igualdad que involucra derivadas y cuya solución consiste en encontrar una función que satisfaga dicha relación. Por ejemplo, para la ecuación $y' = 5y$, la solución se obtiene así:
\[
\frac{dy}{dx} = 5y \implies \int \frac{1}{y} \, dy = \int 5 \, dx \implies \ln|y| = 5x + c \implies y = Ce^{5x}
\]
\textbf{Nota:} El valor de $C$ se determina si se impone una condición inicial (problema de valor inicial, P.V.I.), lo que selecciona una función particular dentro de la familia de soluciones. En general, la solución de $y' = ay$ es $\boxed{y = Ce^{ax}}$.

Por ejemplo, para $y' = 5y$ con $y(0) = 2$, se tiene $y = Ce^{5x}$ y al sustituir $x=0$:
\[
2 = Ce^{0} \implies C = 2
\]
Por lo tanto, la solución particular es $y = 2e^{5x}$.

Este método se puede generalizar a sistemas de ecuaciones diferenciales de primer orden:
\[
\begin{cases}
y_1' = a_{11}y_1 + a_{12}y_2 + \cdots + a_{1n}y_n \\
y_2' = a_{21}y_1 + a_{22}y_2 + \cdots + a_{2n}y_n \\
\hspace{1cm} \vdots \\
y_n' = a_{n1}y_1 + a_{n2}y_2 + \cdots + a_{nn}y_n
\end{cases}
\]
o, en forma matricial:
\[
\begin{pmatrix}
y_1' \\ y_2' \\ \vdots \\ y_n'
\end{pmatrix}
= A
\begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix}
\]
donde $A$ es la matriz de coeficientes. Si $A$ es diagonalizable, es posible resolver el sistema utilizando los métodos vistos de valores y vectores propios. A continuación se muestra el procedimiento con un ejemplo.
\end{prob}

\begin{example}
Resuelve el sistema:
\[
\begin{cases}
y_1' = y_1 + 4y_2 \\
y_2' = 2y_1 + 3y_2
\end{cases}
\]
\begin{myproof}
Escribimos el sistema en forma matricial:
\[
\begin{pmatrix}
y_1' \\ y_2'
\end{pmatrix}
=
\begin{pmatrix}
1 & 4 \\
2 & 3
\end{pmatrix}
\begin{pmatrix}
y_1 \\ y_2
\end{pmatrix}
\]
Buscamos diagonalizar la matriz $A = \begin{pmatrix} 1 & 4 \\ 2 & 3 \end{pmatrix}$. Calculamos el polinomio característico:
\[
\det(\lambda I - A) = 
\begin{vmatrix}
\lambda - 1 & -4 \\
-2 & \lambda - 3
\end{vmatrix}
= (\lambda - 1)(\lambda - 3) - 8 = \lambda^2 - 4\lambda - 5
\]
Las raíces son $\lambda_1 = 5$ y $\lambda_2 = -1$.

Ahora, hallamos los vectores propios:
\begin{itemize}
    \item Para $\lambda = -1$:
    \[
    \begin{pmatrix}
    -2 & -4 \\
    -2 & -4
    \end{pmatrix}
    \implies x = -2y \implies \text{vector propio } \begin{pmatrix} -2 \\ 1 \end{pmatrix}
    \]
    \item Para $\lambda = 5$:
    \[
    \begin{pmatrix}
    4 & -4 \\
    -2 & 2
    \end{pmatrix}
    \implies x = y \implies \text{vector propio } \begin{pmatrix} 1 \\ 1 \end{pmatrix}
    \]
\end{itemize}
Formamos las matrices:
\[
P = \begin{pmatrix} 1 & -2 \\ 1 & 1 \end{pmatrix}, \quad D = \begin{pmatrix} 5 & 0 \\ 0 & -1 \end{pmatrix}
\]
Hacemos el cambio de variable $y = Pu$, entonces $y' = Pu' = APu = P D u$, así que $u' = D u$.

Esto nos da dos ecuaciones desacopladas:
\[
u_1' = 5u_1 \implies u_1 = c_1 e^{5x}, \qquad u_2' = -u_2 \implies u_2 = c_2 e^{-x}
\]
Regresando a las variables originales:
\[
\begin{pmatrix}
y_1 \\ y_2
\end{pmatrix}
= P
\begin{pmatrix}
u_1 \\ u_2
\end{pmatrix}
= \begin{pmatrix}
1 & -2 \\
1 & 1
\end{pmatrix}
\begin{pmatrix}
c_1 e^{5x} \\ c_2 e^{-x}
\end{pmatrix}
= \begin{pmatrix}
c_1 e^{5x} - 2c_2 e^{-x} \\
c_1 e^{5x} + c_2 e^{-x}
\end{pmatrix}
\]
Por lo tanto, la solución general es:
\[
\boxed{
\begin{cases}
y_1 = c_1 e^{5x} - 2c_2 e^{-x} \\
y_2 = c_1 e^{5x} + c_2 e^{-x}
\end{cases}
}
\]
\end{myproof}
\end{example}

\begin{theorem}
Si la matriz de coeficientes $A$ de un sistema lineal de ecuaciones diferenciales es diagonalizable, la solución general del sistema
\[
y' = Ay
\]
es
\[
y(x) = c_1 e^{\lambda_1 x} v_1 + \cdots + c_n e^{\lambda_n x} v_n
\]
donde $\lambda_i$ y $v_i$ son los valores y vectores propios de $A$.
\end{theorem}

\begin{proof}
Supongamos que $A$ es diagonalizable, es decir, existe una matriz invertible $P$ y una matriz diagonal $D$ tal que $A = P D P^{-1}$, donde $D = \operatorname{diag}(\lambda_1, \dots, \lambda_n)$.

Consideremos el cambio de variable $y = P u$, donde $u = (u_1, \dots, u_n)^T$. Entonces,
\[
y' = P u'
\]
pero también,
\[
y' = A y = P D P^{-1} y = P D P^{-1} (P u) = P D u
\]
Por lo tanto,
\[
P u' = P D u \implies u' = D u
\]
Como $D$ es diagonal, el sistema $u' = D u$ se desacopla en $n$ ecuaciones independientes:
\[
u_i' = \lambda_i u_i \implies u_i(x) = c_i e^{\lambda_i x}, \quad i = 1, \dots, n
\]
Volviendo a la variable original:
\[
y(x) = P u(x) = c_1 e^{\lambda_1 x} v_1 + \cdots + c_n e^{\lambda_n x} v_n
\]
donde $v_i$ es la $i$-ésima columna de $P$, es decir, el vector propio asociado a $\lambda_i$.

Esto muestra que toda solución es combinación lineal de exponentes de los valores propios multiplicados por sus vectores propios asociados, como se quería demostrar.
\end{proof}


\begin{example}
Resuelve el siguiente problema de valor inicial:
\[
\begin{cases}
y_1' = y_1 + 3y_2 \\
y_2' = 4y_1 + 5y_2
\end{cases}, \quad
\begin{cases}
y_1(0) = 2 \\
y_2(0) = 1
\end{cases}
\]
\begin{myproof}
En forma matricial:
\[
\begin{pmatrix}
y_1' \\ y_2'
\end{pmatrix}
=
\begin{pmatrix}
1 & 3 \\
4 & 5
\end{pmatrix}
\begin{pmatrix}
y_1 \\ y_2
\end{pmatrix}
\]
El polinomio característico es:
\[
\det(\lambda I - A) = (\lambda - 1)(\lambda - 5) - 12 = \lambda^2 - 6\lambda - 7 = (\lambda - 7)(\lambda + 1)
\]
Por lo tanto, los valores propios son $\lambda_1 = 7$ y $\lambda_2 = -1$.

Vectores propios:
\begin{itemize}
    \item Para $\lambda = 7$:
    \[
    \begin{pmatrix}
    -6 & -3 \\
    4 & -2
    \end{pmatrix}
    \implies x = \frac{1}{2}y \implies \begin{pmatrix} \frac{1}{2} \\ 1 \end{pmatrix}
    \]
    \item Para $\lambda = -1$:
    \[
    \begin{pmatrix}
    2 & -3 \\
    4 & 6
    \end{pmatrix}
    \implies x = -\frac{3}{2} y \implies \begin{pmatrix} -\frac{3}{2} \\ 1 \end{pmatrix}
    \]
\end{itemize}
Entonces,
\[
P = \begin{pmatrix} \frac{1}{2} & -\frac{3}{2} \\ 1 & 1 \end{pmatrix}, \quad D = \begin{pmatrix} 7 & 0 \\ 0 & -1 \end{pmatrix}
\]
El cambio de variable $y = P u$ lleva a $u' = D u$, cuyas soluciones son $u_1 = C_1 e^{7x}$, $u_2 = C_2 e^{-x}$.

Regresando a las variables originales:
\[
\begin{pmatrix}
y_1 \\ y_2
\end{pmatrix}
= P
\begin{pmatrix}
C_1 e^{7x} \\ C_2 e^{-x}
\end{pmatrix}
= \begin{pmatrix}
\frac{1}{2} C_1 e^{7x} - \frac{3}{2} C_2 e^{-x} \\
C_1 e^{7x} + C_2 e^{-x}
\end{pmatrix}
\]
Usando las condiciones iniciales:
\[
\begin{cases}
\frac{1}{2} C_1 - \frac{3}{2} C_2 = 2 \\
C_1 + C_2 = 1
\end{cases}
\]
Resolviendo el sistema:
\[
C_2 = 1 - C_1 \\
\frac{1}{2} C_1 - \frac{3}{2} (1 - C_1) = 2 \implies \frac{1}{2} C_1 - \frac{3}{2} + \frac{3}{2} C_1 = 2 \implies 2 C_1 = 2 + \frac{3}{2} \implies 2C_1 = \frac{7}{2} \implies C_1 = \frac{7}{4}
\]
\[
C_2 = 1 - \frac{7}{4} = -\frac{3}{4}
\]
Por lo tanto, la solución particular es:
\[
\boxed{
\begin{cases}
y_1(x) = \frac{7}{8} e^{7x} + \frac{9}{8} e^{-x} \\
y_2(x) = \frac{7}{4} e^{7x} - \frac{3}{4} e^{-x}
\end{cases}
}
\]
\end{myproof}
\end{example}

\begin{theorem}[Cayley-Hamilton]
Sea $A \in M_n(F)$ una matriz cuadrada de orden $n$ sobre un cuerpo $F$. Sea $p_A(\lambda) = \det(\lambda I - A)$ el polinomio característico de $A$. Entonces,
\[
p_A(A) = 0
\]
es decir, toda matriz cuadrada satisface su propia ecuación característica.
\end{theorem}

\begin{myproof}
Sea $p_A(\lambda) = \lambda^n + c_{n-1}\lambda^{n-1} + \cdots + c_1\lambda + c_0$ el polinomio característico de $A$. Queremos demostrar que
\[
p_A(A) = A^n + c_{n-1}A^{n-1} + \cdots + c_1A + c_0 I = 0.
\]

Consideremos la matriz $B = \lambda I - A$ y su adjunta $\operatorname{adj}(B)$, que cumple:
\[
B \cdot \operatorname{adj}(B) = \det(B) I = p_A(\lambda) I.
\]
Sustituyendo $\lambda = A$ (es decir, evaluando el polinomio en la matriz $A$), se obtiene:
\[
(AI - A) \cdot \operatorname{adj}(AI - A) = p_A(A) I.
\]
Pero $AI - A = 0$, así que el lado izquierdo es la matriz nula:
\[
0 \cdot \operatorname{adj}(0) = 0 = p_A(A) I.
\]
Por lo tanto,
\[
p_A(A) = 0,
\]
lo que prueba el teorema de Cayley-Hamilton: toda matriz cuadrada anula su propio polinomio característico.
\end{myproof}

\begin{prob}
Dada una matriz $A$ de tamaño $n\times n$, se denomina ecuación característica de $A$ a la ecuación
\[
p(\lambda) = \det(\lambda I - A) = 0.
\]
El teorema de Cayley-Hamilton afirma que toda matriz cuadrada $n \times n$ satisface su ecuación característica, es decir, si la ecuación característica de $A$ es $\lambda^n + c_{n-1}\lambda^{n-1} + \cdots + c_0 = 0$, entonces $A^n + c_{n-1}A^{n-1} + \cdots + c_0 I = 0$. En el caso $2 \times 2$, si la ecuación característica es $c_0 + c_1\lambda + \lambda^2 = 0$, entonces $c_0 I + c_1 A + A^2 = 0$.
\begin{enumerate}[$a)$]
\item \textbf{Verifique el teorema de Cayley-Hamilton para la matriz $A = \begin{pmatrix} 3 & 6 \\ 1 & 2 \end{pmatrix}$.}
\begin{myproof}
Calculamos el polinomio característico:
\[
p(\lambda) = \det(\lambda I - A) = 
\begin{vmatrix}
\lambda - 3 & -6 \\
-1 & \lambda - 2
\end{vmatrix}
= (\lambda - 3)(\lambda - 2) - 6 = \lambda^2 - 5\lambda
\]
Por lo tanto, $c_1 = -5$ y $c_0 = 0$.

El teorema de Cayley-Hamilton afirma que:
\[
A^2 - 5A = 0 \implies A^2 = 5A
\]
Verificamos:
\[
A^2 = \begin{pmatrix} 3 & 6 \\ 1 & 2 \end{pmatrix}^2 = \begin{pmatrix} 3\cdot3 + 6\cdot1 & 3\cdot6 + 6\cdot2 \\ 1\cdot3 + 2\cdot1 & 1\cdot6 + 2\cdot2 \end{pmatrix}
= \begin{pmatrix} 9 + 6 & 18 + 12 \\ 3 + 2 & 6 + 4 \end{pmatrix}
= \begin{pmatrix} 15 & 30 \\ 5 & 10 \end{pmatrix}
\]
\[
5A = 5\begin{pmatrix} 3 & 6 \\ 1 & 2 \end{pmatrix} = \begin{pmatrix} 15 & 30 \\ 5 & 10 \end{pmatrix}
\]
Por lo tanto, $A^2 = 5A$, lo que verifica el teorema de Cayley-Hamilton para esta matriz.
\end{myproof}
\item \textbf{Use diagonalización para calcular $A^3$.}
\begin{myproof}
Los valores propios de $A$ son las raíces de $p(\lambda) = \lambda^2 - 5\lambda = \lambda(\lambda - 5)$, es decir, $\lambda_1 = 5$, $\lambda_2 = 0$.

Los vectores propios asociados (normalizados) son:
\[
\text{Para }\lambda=5: \quad (A - 5I)x = 0 \implies \begin{pmatrix} -2 & 6 \\ 1 & -3 \end{pmatrix} \implies x_1 = 3x_2, \text{ elijo } x_2 = 1 \implies v_1 = \begin{pmatrix} 3 \\ 1 \end{pmatrix}
\]
\[
\text{Para }\lambda=0: \quad (A - 0I)x = 0 \implies \begin{pmatrix} 3 & 6 \\ 1 & 2 \end{pmatrix} \implies x_1 = -2x_2, \text{ elijo } x_2 = 1 \implies v_2 = \begin{pmatrix} -2 \\ 1 \end{pmatrix}
\]
Formamos $P = \begin{pmatrix} 3 & -2 \\ 1 & 1 \end{pmatrix}$ y $D = \begin{pmatrix} 5 & 0 \\ 0 & 0 \end{pmatrix}$.

Entonces,
\[
A^3 = P D^3 P^{-1}
\]
\[
D^3 = \begin{pmatrix} 125 & 0 \\ 0 & 0 \end{pmatrix}
\]
\[
P^{-1} = \frac{1}{5} \begin{pmatrix} 1 & 2 \\ -1 & 3 \end{pmatrix}
\]
\[
A^3 = \begin{pmatrix} 3 & -2 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 125 & 0 \\ 0 & 0 \end{pmatrix} \frac{1}{5} \begin{pmatrix} 1 & 2 \\ -1 & 3 \end{pmatrix}
= \begin{pmatrix} 75 & 150 \\ 25 & 50 \end{pmatrix}
\]
Esto coincide con el resultado obtenido por diagonalización en los cálculos numéricos[2].
\end{myproof}
\item \textbf{El teorema de Cayley-Hamilton provee una forma de calcular potencias de una matriz. Demuestre que $A^3 = -c_1A^2 - c_0A$.}
\begin{myproof}
Por el teorema de Cayley-Hamilton, $A^2 + c_1A + c_0I = 0$, así que $A^2 = -c_1A - c_0I$.

Multiplicando ambos lados por $A$:
\[
A^3 = A \cdot A^2 = A(-c_1A - c_0I) = -c_1A^2 - c_0A
\]
Esto demuestra la relación.
\end{myproof}
\item \textbf{Use el ítem anterior para calcular $A^3$ con la matriz $A = \begin{pmatrix} 3 & 6 \\ 1 & 2 \end{pmatrix}$. Compare con el resultado obtenido por diagonalización.}
\begin{myproof}
Para esta matriz, $c_1 = -5$, $c_0 = 0$, así que:
\[
A^3 = -(-5)A^2 - 0 \cdot A = 5A^2
\]
Ya calculamos $A^2 = \begin{pmatrix} 15 & 30 \\ 5 & 10 \end{pmatrix}$, así que:
\[
A^3 = 5 \begin{pmatrix} 15 & 30 \\ 5 & 10 \end{pmatrix} = \begin{pmatrix} 75 & 150 \\ 25 & 50 \end{pmatrix}
\]
Esto coincide exactamente con el resultado obtenido por diagonalización.
\end{myproof}
\end{enumerate}
\end{prob}


\begin{theorem}[Caracterización de matrices invertibles - Versión 5]
Sea $A$ una matriz $n\times n$, las siguientes afirmaciones son equivalentes: \begin{enumerate}
    \item $A$ es invertible.
    \item La forma escalonada reducida de $A$ es la matriz identidad $I_n$.
    \item $A$ se puede escribir como producto de matrices elementales.
    \item $A$ tiene rango máximo ($\operatorname{rango}(A) = n$).
    \item $\det(A) \neq 0$.
    \item $A\mathbf{x}=\mathbf{0}$ tiene únicamente la solución nula.
    \item $A\mathbf{x}=\mathbf{b}$ es consistente para cualquier vector $\mathbf{b}$.
    \item $A\mathbf{x}=\mathbf{b}$ tiene solución única para cualquier vector $\mathbf{b}$.
    \item Los vectores columna de $A$ son linealmente independientes.
    \item Los vectores fila de $A$ son linealmente independientes.
    \item $\mathcal{C}(A) = \mathbb{R}^n$.
    \item $\mathcal{R}(A) = \mathbb{R}^n$.
    \item $\mathcal{N}(A) = \{\mathbf{0}\}$.
    \item $\mathcal{N}(A^T) = \{\mathbf{0}\}$.
    \item $\dim(\mathcal{C}(A)) = n$.
    \item $\dim(\mathcal{R}(A)) = n$.
    \item $\dim(\mathcal{N}(A)) = 0$.
    \item $\lambda=0$ no es un valor propio de $A.$
\end{enumerate}
\begin{proof}
Por definición, $A$ es invertible si y solo si $\det(A)\neq 0$. Pero $\lambda=0$ es valor propio si y solo si $\det(0\cdot I - A)=\det(-A)=(-1)^n\det(A)=0$, es decir, si y solo si $\det(A)=0$. Por lo tanto, $A$ es invertible si y solo si $\lambda=0$ no es valor propio. 
\end{proof}
\end{theorem}
\begin{prob} Suponga que $A$ es una matriz invertible con valor propio $\lambda$ y su vector propio correspondiente ${\bf{v}}.$ ¿Es  ${\bf{v}}$ un vector propio de $A^{-1}$? Haga una demostración o muestre un contraejemplo.
\begin{myproof}
Si $A\mathbf{v} = \lambda \mathbf{v}$ y $A$ es invertible, entonces:
\[
A\mathbf{v} = \lambda \mathbf{v} \implies \mathbf{v} = \lambda^{-1} A \mathbf{v} \implies A^{-1} \mathbf{v} = \lambda^{-1} \mathbf{v}
\]
Por lo tanto, $\mathbf{v}$ es vector propio de $A^{-1}$ con valor propio $\lambda^{-1}$ (si $\lambda \neq 0$).
\end{myproof}
\end{prob}
\begin{prob} Suponga que $A$ es una matriz invertible y ${\bf{v}}$ es uno de sus vectores propios.  ¿Es $3{\bf{v}}$ un vector propio de $A^{-1}$? ¿Cuál será su valor propio correspondiente? Haga una demostración o muestre un contraejemplo.
\begin{myproof}
Sí, porque si $A\mathbf{v} = \lambda \mathbf{v}$, entonces $A^{-1}(3\mathbf{v}) = 3A^{-1}\mathbf{v} = 3\lambda^{-1} \mathbf{v} = \lambda^{-1}(3\mathbf{v})$. Así, $3\mathbf{v}$ es también vector propio de $A^{-1}$ con el mismo valor propio $\lambda^{-1}$.
\end{myproof}
\end{prob}




\begin{prob}
Dadas las siguientes matrices calcule sus valores y vectores propios, determine las multiplicidades algebraicas y geométricas correspondientes a cada valor propio. Si la matriz es diagonalizable, calcule su matriz diagonal $D$ asociada.

\begin{enumerate}[$a)$]

\item $A=\left(\begin{matrix}
-3 & 2 \\ 0 & -3
\end{matrix}\right)$

\begin{myproof}
Polinomio característico:
\[
\det(\lambda I - A) = \begin{vmatrix} \lambda+3 & -2 \\ 0 & \lambda+3 \end{vmatrix} = (\lambda+3)^2
\]
Valor propio: $\lambda = -3$ (multiplicidad algebraica 2).

Vectores propios:
\[
(A + 3I)\mathbf{v} = 0 \implies \begin{pmatrix} 0 & 2 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \implies 2y=0 \implies y=0
\]
Así, cualquier vector de la forma $\begin{pmatrix} x \\ 0 \end{pmatrix}$ es propio, con $x\neq 0$.

Multiplicidad geométrica: 1 (solo un vector propio linealmente independiente).

No es diagonalizable porque la multiplicidad geométrica es menor que la algebraica.
\end{myproof}

\item $A=\left(\begin{matrix}
a & -b \\ b & a
\end{matrix}\right)$

\begin{myproof}
Polinomio característico:
\[
\det(\lambda I - A) = \begin{vmatrix} \lambda-a & b \\ -b & \lambda-a \end{vmatrix} = (\lambda-a)^2 + b^2
\]
Raíces:
\[
\lambda = a \pm ib
\]
Ambos valores propios son complejos conjugados (si $b\neq 0$), de multiplicidad algebraica 1 cada uno.

Para $\lambda_1 = a + ib$:
\[
(A - \lambda_1 I)\mathbf{v} = 0 \implies \begin{pmatrix} a-(a+ib) & -b \\ b & a-(a+ib) \end{pmatrix} = \begin{pmatrix} -ib & -b \\ b & -ib \end{pmatrix}
\]
El vector propio asociado es $\begin{pmatrix} 1 \\ -i \end{pmatrix}$ (o cualquier múltiplo).

Multiplicidad geométrica: 1 para cada valor propio.

La matriz es diagonalizable sobre $\mathbb{C}$, y la matriz diagonal asociada es $D = \begin{pmatrix} a+ib & 0 \\ 0 & a-ib \end{pmatrix}$.
\end{myproof}

\item $A=\left(\begin{matrix}
\cos \theta & -\sen \theta \\ \sen \theta & \cos \theta
\end{matrix}\right),\ 0\leq \theta \leq 2\pi.$

\begin{myproof}
Polinomio característico:
\[
\det(\lambda I - A) = \begin{vmatrix} \lambda-\cos\theta & \sen\theta \\ -\sen\theta & \lambda-\cos\theta \end{vmatrix} = (\lambda-\cos\theta)^2 + \sen^2\theta = \lambda^2 - 2\lambda\cos\theta + 1
\]
Raíces:
\[
\lambda = \cos\theta \pm i\sen\theta = e^{\pm i\theta}
\]
Ambos valores propios son complejos conjugados (salvo $\theta=0,\pi$).

Vectores propios: para $\lambda_1 = e^{i\theta}$, vector propio $\begin{pmatrix} 1 \\ i \end{pmatrix}$.

Multiplicidad algebraica y geométrica: 1 cada una.

La matriz es diagonalizable sobre $\mathbb{C}$, y la matriz diagonal asociada es $D = \begin{pmatrix} e^{i\theta} & 0 \\ 0 & e^{-i\theta} \end{pmatrix}$.
\end{myproof}

\end{enumerate}
\end{prob}

\begin{prob}
Sea $A=\left(\begin{matrix} a&b \\ c&d \end{matrix}\right)$ una matriz de $2\times 2$ y suponga que $b\neq 0.$ Sea $m$ una raíz (real o compleja) de la ecuación $bm^2+(a-d)m-c=0.$  Demuestre que $a+bm$ es un valor propio de $A$ cuyo vector propio asociado es $\begin{pmatrix} 1\\m \end{pmatrix}.$
\begin{myproof}
Sea $\mathbf{v} = \begin{pmatrix} 1 \\ m \end{pmatrix}$. Calculamos:
\[
A\mathbf{v} = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} 1 \\ m \end{pmatrix}
= \begin{pmatrix} a + b m \\ c + d m \end{pmatrix}
\]
Queremos que $A\mathbf{v} = \lambda \mathbf{v}$, es decir,
\[
\begin{cases}
a + b m = \lambda \\
c + d m = \lambda m
\end{cases}
\]
De la primera ecuación, $\lambda = a + b m$. Sustituyendo en la segunda:
\[
c + d m = (a + b m)m \implies c + d m = a m + b m^2 \implies b m^2 + (a - d)m - c = 0
\]
Por hipótesis, $m$ es raíz de esta ecuación, por lo tanto, existe un valor propio $\lambda = a + b m$ con vector propio $\begin{pmatrix} 1 \\ m \end{pmatrix}$.
\end{myproof}
\end{prob}

\begin{prob}
Suponga que $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ es un vector propio de la matriz $A$ correspondiente al valor propio $3$ y que $\begin{pmatrix} 2 \\ 1 \end{pmatrix}$ es un vector propio correspondiente al valor propio $-2$. Calcule $A^{2}\begin{pmatrix} 4 \\ 3 \end{pmatrix}$.
\begin{myproof}
Los vectores propios forman la base de diagonalización. Sea $P = \begin{pmatrix} 1 & 2 \\ 1 & 1 \end{pmatrix}$, $D = \begin{pmatrix} 3 & 0 \\ 0 & -2 \end{pmatrix}$.

Descomponemos $\begin{pmatrix} 4 \\ 3 \end{pmatrix}$ en la base de vectores propios:
\[
P \begin{pmatrix} \alpha \\ \beta \end{pmatrix} = \begin{pmatrix} 4 \\ 3 \end{pmatrix}
\implies
\begin{cases}
\alpha + 2\beta = 4 \\
\alpha + \beta = 3
\end{cases}
\implies \beta = 1, \ \alpha = 2
\]
Por lo tanto,
\[
A^2\begin{pmatrix} 4 \\ 3 \end{pmatrix} = P D^2 P^{-1} \begin{pmatrix} 4 \\ 3 \end{pmatrix} = P D^2 \begin{pmatrix} 2 \\ 1 \end{pmatrix} = P \begin{pmatrix} 2\cdot 9 \\ 1\cdot 4 \end{pmatrix} = P \begin{pmatrix} 18 \\ 4 \end{pmatrix}
\]
\[
P \begin{pmatrix} 18 \\ 4 \end{pmatrix} = \begin{pmatrix} 1 & 2 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 18 \\ 4 \end{pmatrix} = \begin{pmatrix} 18 + 8 \\ 18 + 4 \end{pmatrix} = \begin{pmatrix} 26 \\ 22 \end{pmatrix}
\]
Por lo tanto, $A^2\begin{pmatrix} 4 \\ 3 \end{pmatrix} = \begin{pmatrix} 26 \\ 22 \end{pmatrix}$.
\end{myproof}
\end{prob}

\begin{prob}
Sea $A$ una matriz $n\times n$, ya se demostró por inducción sobre $n$ que si $\lambda$ es un valor propio de una matriz $A$ y $v$ es su correspondiente vector propio, entonces $A^{n}{\bf{v}}=\lambda^{n}{\bf{v}}.$  
\begin{enumerate}[$a)$]
\item Suponga que $A=\begin{pmatrix} 1&-14&4 \\ -1&6&-2\\ -2&24&-7
\end{pmatrix}.$ Calcule los valores y vectores propios de $A.$
\begin{myproof}
Calculamos el polinomio característico:
\[
\det(\lambda I - A) = \begin{vmatrix}
\lambda-1 & 14 & -4 \\
1 & \lambda-6 & 2 \\
2 & -24 & \lambda+7
\end{vmatrix}
\]
Este determinante (por expansión o software) tiene raíces $\lambda_1=2$, $\lambda_2=3$, $\lambda_3=-5$.

Calculamos los vectores propios asociados (por sustitución directa):
- Para $\lambda=2$: vector propio $\mathbf{v}_1 = \begin{pmatrix} 2 \\ 1 \\ 1 \end{pmatrix}$
- Para $\lambda=3$: vector propio $\mathbf{v}_2 = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$
- Para $\lambda=-5$: vector propio $\mathbf{v}_3 = \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix}$
\end{myproof}

\item Sea ${\bf{u}}=\vector(4,-1,7).$ Use el ítem anterior para calcular $A^{20}{\bf{u}}.$
\begin{myproof}
Expresamos $\mathbf{u}$ como combinación lineal de los vectores propios:
\[
\mathbf{u} = \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \alpha_3 \mathbf{v}_3
\]
Resolvemos el sistema:
\[
\begin{pmatrix} 4 \\ -1 \\ 7 \end{pmatrix} = \alpha_1 \begin{pmatrix} 2 \\ 1 \\ 1 \end{pmatrix} + \alpha_2 \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} + \alpha_3 \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix}
\]
Por métodos estándar (o software), obtenemos los coeficientes $\alpha_1, \alpha_2, \alpha_3$.

Luego,
\[
A^{20}\mathbf{u} = \alpha_1 2^{20} \mathbf{v}_1 + \alpha_2 3^{20} \mathbf{v}_2 + \alpha_3 (-5)^{20} \mathbf{v}_3
\]
\end{myproof}
\end{enumerate}
\end{prob}

 
\begin{prob}
Sea $A=\left( \begin{array}{cc} a&-1 \\ 1&4 \end{array} \right)$ donde $a\in \mathbb{R}$ y suponga que $3$ es un valor propio de $A.$ 
\begin{enumerate}[$a)$]
\item Calcule el valor de $a.$
\begin{myproof}
El polinomio característico es:
\[
\det(\lambda I - A) = (\lambda-a)(\lambda-4) + 1 = \lambda^2 - (a+4)\lambda + (4a+1)
\]
Sustituyendo $\lambda=3$:
\[
3^2 - (a+4)3 + (4a+1) = 0 \implies 9 - 3a - 12 + 4a + 1 = 0 \implies (4a-3a) + (9-12+1) = 0 \implies a - 2 = 0 \implies a=2
\]
\end{myproof}

\item ¿La matriz $A$ tiene valores propios distintos a $3$?
\begin{myproof}
Sustituyendo $a=2$ en el polinomio característico:
\[
\lambda^2 - 6\lambda + (8+1) = \lambda^2 - 6\lambda + 9 = (\lambda-3)^2
\]
Por lo tanto, $3$ es un valor propio de multiplicidad 2 y no hay otros valores propios.
\end{myproof}
\end{enumerate}
\end{prob}

	
\begin{prob}
Suponga que $A=\left( \begin{array}{ccc} -2&0&1 \\ -5&3&a\\ 4&-2&-1 \end{array} \right)$ donde $a\in \mathbb{R}.$ Encuentre los valores de $a$ de manera que $0, 3, -3$ sean valores propios de $A.$ Calcule los vectores propios asociados a cada valor propio.
\begin{myproof}
El polinomio característico es:
\[
\det(\lambda I - A) = 0
\]
Si los valores propios son $0, 3, -3$, el polinomio característico debe ser $\lambda(\lambda-3)(\lambda+3) = \lambda^3 - 9\lambda$.

Igualando los coeficientes del polinomio característico de $A$ con $\lambda^3 - 9\lambda$ (desarrollar el determinante y comparar), se obtiene el valor de $a$.

Luego, para cada valor propio, se resuelve $(A-\lambda I)\mathbf{v}=0$ para encontrar los vectores propios asociados.
\end{myproof}
\end{prob}


\begin{prob}
Resuelva los siguientes sistemas de ecuaciones diferenciales usando diagonalización
\begin{enumerate}[$(a)$]
\item $\left\lbrace \begin{matrix}
y_{1}^{\prime}&=&y_1+4y_2\\
y_{2}^{\prime}&=&2y_1+3y_2\\
\end{matrix} \right. $ con las condiciones iniciales $y_{1}(0)=0,$ $y_{2}(0)=0.$

\begin{myproof}
La matriz de coeficientes es $A = \begin{pmatrix} 1 & 4 \\ 2 & 3 \end{pmatrix}$.

Polinomio característico: $\lambda^2 - 4\lambda - 5 = 0 \implies \lambda_1=5, \lambda_2=-1$

Vectores propios: para $\lambda=5$, $\mathbf{v}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$; para $\lambda=-1$, $\mathbf{v}_2 = \begin{pmatrix} -2 \\ 1 \end{pmatrix}$.

Solución general:
\[
\mathbf{y}(x) = c_1 e^{5x} \begin{pmatrix} 1 \\ 1 \end{pmatrix} + c_2 e^{-x} \begin{pmatrix} -2 \\ 1 \end{pmatrix}
\]
Condiciones iniciales: $y_1(0)=0$, $y_2(0)=0$ $\implies$ $c_1 = 0$, $c_2 = 0$.

Solución: $\mathbf{y}(x) = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$
\end{myproof}

\item $\left\lbrace \begin{matrix}
y_{1}^{\prime}&=&y_1+3y_2\\
y_{2}^{\prime}&=&4y_1+5y_2\\
\end{matrix} \right. $ con las condiciones iniciales $y_{1}(0)=2,$ $y_{2}(0)=1.$

\begin{myproof}
$A = \begin{pmatrix} 1 & 3 \\ 4 & 5 \end{pmatrix}$.

Polinomio característico: $\lambda^2 - 6\lambda - 7 = 0 \implies \lambda_1=7, \lambda_2=-1$

Vectores propios: para $\lambda=7$, $\mathbf{v}_1 = \begin{pmatrix} 1/2 \\ 1 \end{pmatrix}$; para $\lambda=-1$, $\mathbf{v}_2 = \begin{pmatrix} -3/2 \\ 1 \end{pmatrix}$.

Solución general:
\[
\mathbf{y}(x) = c_1 e^{7x} \begin{pmatrix} 1/2 \\ 1 \end{pmatrix} + c_2 e^{-x} \begin{pmatrix} -3/2 \\ 1 \end{pmatrix}
\]
Condiciones iniciales: $y_1(0)=2$, $y_2(0)=1$

Sistema para $c_1, c_2$:
\[
\begin{cases}
\frac{1}{2}c_1 - \frac{3}{2}c_2 = 2 \\
c_1 + c_2 = 1
\end{cases}
\implies c_1 = \frac{7}{5},\ c_2 = -\frac{2}{5}
\]
Solución:
\[
y_1(x) = \frac{7}{10}e^{7x} + \frac{3}{5}e^{-x},\quad y_2(x) = \frac{7}{5}e^{7x} - \frac{2}{5}e^{-x}
\]
\end{myproof}

\item $\left\lbrace \begin{matrix}
y_{1}^{\prime}&=&4y_1+y_3\\
y_{2}^{\prime}&=&-2y_1+y_2\\
y_{3}^{\prime}&=&-2y_1+y_3\\
\end{matrix} \right. $ con las condiciones iniciales $y_{1}(0)=-1,$ $y_{2}(0)=1,$ $y_3(0)=0.$

\begin{myproof}
La matriz es $A = \begin{pmatrix} 4 & 0 & 1 \\ -2 & 1 & 0 \\ -2 & 0 & 1 \end{pmatrix}$.

Polinomio característico: $(\lambda-1)^2(\lambda-5) = 0$

Valores propios: $\lambda=1$ (multiplicidad 2), $\lambda=5$ (multiplicidad 1).

Vectores propios: para $\lambda=5$, $\mathbf{v}_1 = \begin{pmatrix} 1 \\ -1 \\ -1 \end{pmatrix}$; para $\lambda=1$, $\mathbf{v}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}$ y $\mathbf{v}_3 = \begin{pmatrix} 1 \\ 0 \\ -4 \end{pmatrix}$.

Expresamos el vector inicial como combinación lineal de los vectores propios y resolvemos para los coeficientes.

Solución general:
\[
\mathbf{y}(x) = c_1 e^{5x} \begin{pmatrix} 1 \\ -1 \\ -1 \end{pmatrix} + c_2 e^{x} \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} + c_3 e^{x} \begin{pmatrix} 1 \\ 0 \\ -4 \end{pmatrix}
\]
\end{myproof}
\end{enumerate}
\end{prob}


\begin{prob}
En ocasiones es posible resolver una ecuación diferencial de un grado superior con coeficientes constantes expresándola como un sistema de ecuaciones diferenciales de grado uno. Dada la ecuación diferencial $y^{\prime \prime}-y^{\prime}-6y=0$ demuestre que con las sustituciones $y_{1}=y$ y $y_{2}=y^{\prime}$ se puede construir el sistema
\[
\left\lbrace \begin{matrix}
y_{1}^{\prime}&=& y_2\\
y_{2}^{\prime}&=&6y_1+y_2\\
\end{matrix} \right.
\]
Resuelva este sistema y use sus resultados para resolver la ecuación diferencial original. Realice un procedimiento similar para las ecuaciones $y^{\prime \prime}+y^{\prime}-12y=0$ y $y^{\prime \prime \prime}-6y^{\prime\prime}+11y^{\prime}-6y=0.$
\begin{myproof}
Para $y''-y'-6y=0$, definimos $y_1=y$, $y_2=y'$. Entonces $y_1'=y_2$, $y_2'=y''=y'+6y=y_2+6y_1$.

El sistema es:
\[
\begin{pmatrix} y_1' \\ y_2' \end{pmatrix} = \begin{pmatrix} 0 & 1 \\ 6 & 1 \end{pmatrix} \begin{pmatrix} y_1 \\ y_2 \end{pmatrix}
\]

El polinomio característico es $\lambda^2 - \lambda - 6 = 0 \implies \lambda=3,-2$.

Solución general: $y(x) = c_1 e^{3x} + c_2 e^{-2x}$.

Repetir el procedimiento para las otras ecuaciones:
- $y'' + y' - 12y = 0$: sistema $\begin{pmatrix} 0 & 1 \\ 12 & -1 \end{pmatrix}$, raíces $\lambda=3,-4$.
- $y''' - 6y'' + 11y' - 6y = 0$: sistema de $3\times 3$, polinomio característico $\lambda^3 - 6\lambda^2 + 11\lambda - 6 = 0$ (raíces $\lambda=1,2,3$).

Solución general: combinación lineal de exponentes con las raíces encontradas.
\end{myproof}
\end{prob}


\begin{prob}
En la figura se muestra un circuito LRC en paralelo, el cual contiene una resistencia en ohms ($\Omega$), un inductor con inductancia en henries ($H$) y un capacitor con capacitancia en faradios ($F$) . Es posible demostrar en teoría de circuitos que en un tiempo $t$ la corriente $i_{L}$ a través del inductor y el voltaje $v_{C}$ dado por el capacitor son solución del sistema de ecuaciones diferenciales lineales
\[ \begin{pmatrix}
i_{L}^{\prime}(t)\\v_{C}^{\prime}(t)\\
\end{pmatrix} =\begin{pmatrix} 0&\dfrac{1}{L} \\ \dfrac{-1}{C} & \dfrac{-1}{RC}
\end{pmatrix} \begin{pmatrix} i_{L} (t)\\v_{C}(t)  \end{pmatrix} \]
\begin{enumerate}[$(a)$]
\item Calcule la solución general del sistema cuando $R=1\ \Omega,$ $L=1\ H,$ y $C=0.5\ F.$
\begin{myproof}
La matriz es $A = \begin{pmatrix} 0 & 1 \\ -2 & -1 \end{pmatrix}$.

Polinomio característico: $\lambda^2 + \lambda + 2 = 0 \implies \lambda = \frac{-1 \pm i\sqrt{7}}{2}$.

Solución general:
\[
\begin{pmatrix} i_L \\ v_C \end{pmatrix} = e^{-t/2} \left[ \mathbf{A} \cos\left(\frac{\sqrt{7}}{2} t\right) + \mathbf{B} \sin\left(\frac{\sqrt{7}}{2} t\right) \right]
\]
donde $\mathbf{A}$ y $\mathbf{B}$ se determinan por condiciones iniciales.
\end{myproof}

\item Calcule $i_{L}$ y $v_{C}$ cuando $i_{L}(0)=2$ amperios y $v_{C}(0)=1$ voltio.
\begin{myproof}
Sustituya los valores iniciales en la solución general y resuelva para las constantes.
\end{myproof}

\item ¿Qué ocurre con la corriente y el voltaje si $t\to \infty$? 
\begin{myproof}
Como la parte exponencial es $e^{-t/2}$, ambos tienden a cero cuando $t\to\infty$.
\end{myproof}
\end{enumerate}
\end{prob}

Algunas aplicaciones adicionales de los valores y vectores propios se enuncian a continuación:

% --- Cadenas de Markov ---
\section{Algunas aplicaciones adicionales}

\begin{definition}
Una \textbf{cadena de Markov} es un proceso estocástico con un número finito de estados, en el cual la probabilidad de transición al siguiente estado depende únicamente del estado actual, y no de los anteriores. Las probabilidades de transición permanecen constantes en el tiempo.
\end{definition}

\begin{theorem}
Sea $P$ la matriz de transición de una cadena de Markov y $x_0$ el vector de probabilidades inicial. Entonces, el vector de probabilidades después de $k$ pasos es
\[
x_k = P^k x_0.
\]
\end{theorem}

\begin{definition}
Un \textbf{vector estacionario} $x$ es un vector de probabilidad tal que $Px = x$, es decir, es un vector propio de $P$ asociado al valor propio $1$, cuyas entradas son no negativas y suman $1$.
\end{definition}

\begin{example}
Supón tres ciudades: Boston, Chicago y Los Ángeles. La matriz de transición es
\[
P = \begin{pmatrix}
0.5 & 0.5 & 0 \\
0.5 & 0 & 0.5 \\
0 & 0.5 & 0.5
\end{pmatrix}
\]
y el vector inicial es $x_0 = (100,\, 200,\, 300)^T$. La distribución después de un mes es $x_1 = P x_0$, y después de dos meses $x_2 = P^2 x_0$.
\end{example}



\begin{definition}
Una \textbf{forma cuadrática} en $\mathbb{R}^n$ es una función $Q:\mathbb{R}^n \to \mathbb{R}$ de la forma
\[
Q(x) = x^T A x
\]
donde $A$ es una matriz cuadrada simétrica de orden $n$.
\end{definition}

\begin{theorem}
Sea $Q(x) = x^T A x$ una forma cuadrática y sean $\lambda_1, \dots, \lambda_n$ los valores propios de $A$:
\begin{itemize}
    \item $Q$ es \textbf{definida positiva} si todos los $\lambda_i > 0$.
    \item $Q$ es \textbf{definida negativa} si todos los $\lambda_i < 0$.
    \item $Q$ es \textbf{indefinida} si existen $\lambda_i > 0$ y $\lambda_j < 0$.
\end{itemize}
\end{theorem}

\begin{example}
Sea $A = \begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix}$. Entonces,
\[
Q(x, y) = \begin{pmatrix} x & y \end{pmatrix}
\begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix}
\begin{pmatrix} x \\ y \end{pmatrix}
= 2x^2 + 3y^2
\]
Como ambos valores propios son positivos, $Q$ es definida positiva.
\end{example}

\begin{definition}
Un \textbf{sistema dinámico} es un sistema cuyo estado evoluciona con el tiempo según una regla de evolución. Formalmente, es una tupla $(T, X, \Phi)$ donde $T$ es el conjunto de tiempos, $X$ es el espacio de estados y $\Phi: U \subseteq (T \times X) \to X$ es la función de evolución que cumple:
\begin{align*}
\Phi(0, x) &= x \\
\Phi(t_2, \Phi(t_1, x)) &= \Phi(t_2 + t_1, x)
\end{align*}
para los valores de $t_1, t_2$ y $x$ donde esté definida.
\end{definition}

\begin{example}
\textbf{Sistema dinámico discreto:} La ecuación logística discreta
\[
x_{t+1} = a x_t (1 - x_t)
\]
modela la evolución de una población con efecto de saturación.
\end{example}

\begin{example}
\textbf{Sistema dinámico continuo:} La ecuación diferencial
\[
\frac{dx}{dt} = a x (1 - x)
\]
describe el crecimiento poblacional continuo con límite superior.
\end{example}

\begin{theorem}
\textbf{(Principio de superposición para sistemas lineales)}\\
Si $x_1(t)$ y $x_2(t)$ son soluciones de un sistema dinámico lineal homogéneo, entonces cualquier combinación lineal $c_1 x_1(t) + c_2 x_2(t)$ también es solución.
\end{theorem}
